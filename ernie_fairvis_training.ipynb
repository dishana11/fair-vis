{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"HuggingFaceM4/FairFace\", \"0.25\")\n",
    "print(dataset)\n",
    "\n",
    "fairface_300 = dataset[\"train\"].shuffle(seed=42).select(range(300))\n",
    "print(len(fairface_300))\n",
    "\n",
    "sample = fairface_300[0]\n",
    "print(sample.keys())\n",
    "\n",
    "ernie_data = []\n",
    "\n",
    "for sample in fairface_300:\n",
    "    gender = sample[\"gender\"]\n",
    "    race = sample[\"race\"]\n",
    "    prompt = \"Describe the person in the image in a neutral and factual way.\"\n",
    "    response = f\"The image shows a {gender} person of {race} ethnicity.\"\n",
    "    ernie_data.append({\n",
    "        \"instruction\": prompt,\n",
    "        \"output\": response\n",
    "    })\n",
    "\n",
    "# Save for Unsloth\n",
    "with open(\"fairvis_ernie_sft.json\", \"w\") as f:\n",
    "    json.dump(ernie_data, f, indent=2)\n",
    "\n",
    "print(\"âœ… ERNIE fine-tuning dataset ready:\", len(ernie_data))\n",
    "\n",
    "model_name = \"baidu/ERNIE-4.5-0.3B-PT\"\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# Quick inference test\n",
    "inputs = tokenizer(\"Hello, describe fairness:\", return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"fairvis_ernie_sft.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "print(dataset[0])\n",
    "\n",
    "def tokenize_function(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    output = example[\"output\"]\n",
    "\n",
    "    if isinstance(instruction, list):\n",
    "        instruction = \" \".join(instruction)\n",
    "    if isinstance(output, list):\n",
    "        output = \" \".join(output)\n",
    "\n",
    "    encodings = tokenizer(\n",
    "        instruction + \" \" + output,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
    "    return encodings\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ernie_fairvis\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Pick the first sample\n",
    "sample = fairface_300[0]\n",
    "\n",
    "# Check keys\n",
    "print(sample.keys())\n",
    "\n",
    "# Get the image\n",
    "image = sample[\"image\"]\n",
    "\n",
    "# Display it\n",
    "image.show()\n",
    "\n",
    "# Load BLIP Mini\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Pick a sample image\n",
    "sample = fairface_300[0]\n",
    "image = sample[\"image\"]\n",
    "\n",
    "# Generate caption\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "out = blip_model.generate(**inputs)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"BLIP caption:\", caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}